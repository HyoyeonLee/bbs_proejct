2023.04.14
BBS프로젝트



요약

* 기존 3D 키넥트 카메라로 촬영, 추출한 스켈레톤 정보로 훈련시킨 RF모델을
  일반 안드로이드폰 카메라나 웹캠으로 촬영된 영상 데이터에 적용시켜도 되는가.
  기존 스켈레톤의 joint와 중에서 미디어파이프로 찾은 joint중에 겹치는 13개의 joint만 남기자. 
  그런데 키넥트의 스켈레톤의 좌표가 depth이미지의 좌표계를 따른 것이었네.
  RGB의 좌표계로 좌표변환 시키자.
  
  
    (1) GUI이용해서 depth이미지 (512x512p) 에서 color이미지 (1280x720p) 기반의 coord.로 변환 
        - 좌표변환 시, 카메라 자체가 필요(calibration관련 정보가 카메라마다 다르기 때문)
        - 그런데 촬영당시 사용한 카메라가 아닌 연구소에 있던 키넥트 카메라 기반으로 변환.
        
        - HOW?
        ************************************************************************
        https://github.com/Deep-In-Sight/Kinect_BBS_demo/tree/image_acquisition
        >> bbsQt/qtgui/qobj/QThreadobj.py에서 다음만 변경함
            ( 변경1 @ line 112 : joint = body_grame.ex_joints(s_image,dest_camera=1)
            ( 변경2 @ line 153 : hout = self.stackJoint.copy()
            ( 변경3 @ line 173 : pickle.dump(hout,open(bt_path,"wb"))
        *************************************************************************
        결과 RGB좌표의 스켈레톤 정보는 pickle파일로 저장됨.
        
        
    (2) 미디어파이프와 일치?
        - 결과는 미디어파이프로 찾은 랜드마크정보와 비교해봄 : 잘 안맞음. 전반적인 shift 및 신발/바닥 구별하지 못함.
        - 이러한 불일치가 혹시 좌표변환에 사용된 값들이 촬영당시 카메라 것이 아니라서 생긴것인지 확인해야.
      
    (3) 촬영당시의 키넥트 카메라를 재현해야?
        - 소지하고 있는 카메라로 직접 촬영, 스켈레톤 정보 추출 (RGB좌표계로)
        - 미디어파이프 결과와 비교 : shift는 없어졌으나 신발/바닥 구별 여전히 모호.
        - 신발/바닥 구별에 대해서는 키넥트 결과끼리도 일관성이 없음. 따라서 일괄적인 scaling이 불적합.
      ==> 훈련데이터 촬영에 사용했던 카메라를 훈련데이터의 좌표변환에 사용한 카메라로 재현할 기준모호해서 안하기로.
     
     
     
     ---------------------------------------------------절취선---------------------------------------------------------
     
    (4) 그렇다고 좌표들을 그대로 사용하면 안됨.
        - user들은 해상도가 제각각인 영상을 사용하므로, origin 위치와 normalize길이로 좌표변환 해야함.
        - frame모두를 그대로 사용하지 않고 5frame씩의 평균을 사용.
        - 그 외 등등의 전처리에 해당하는 작업 수행 (14 April?)
        
    (5) split
        - 전체 데이터 중에서 RGB 영상 확보되어 있는 10% 만 test(validation)데이터로 하고. (1명이상이 안잡히는 경우를 우선시해야됨...)
        - 나머지 90%는 훈련데이터로 함.            
    (6) RF모델의 Depth파라미터를 1~2정도 높여도 됨. 
    
